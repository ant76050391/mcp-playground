#!/usr/bin/env python3
"""RAG MCP Server - Simple implementation."""

import asyncio
import hashlib
import json
import logging
import os
import platform
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime

from llama_cpp import Llama
from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.models import ServerCapabilities
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
)

# Text extraction module
from text_extraction import TextExtractionEngine

# Vector store module
from vector_store import ChromaDBVectorStore

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BGEEmbeddingModel:
    """BGE-M3 Korean embedding model wrapper using llama-cpp-python."""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
    
    def load(self):
        """Load the GGUF model with optimized settings."""
        try:
            # Log system information
            cpu_cores = os.cpu_count()
            logger.info(f"System: {platform.system()} {platform.release()}")
            logger.info(f"CPU cores available: {cpu_cores}")
            logger.info(f"Loading BGE-M3 Korean model from: {self.model_path}")
            
            self.model = Llama(
                model_path=self.model_path,
                embedding=True,      # Enable embedding mode
                n_ctx=8192,          # Context window (match model training size)
                verbose=False,       # Minimize logs
                n_threads=cpu_cores, # Use all CPU cores
                n_batch=512,         # Batch size optimization
                use_mmap=True,       # Memory mapping for faster loading
                use_mlock=False,     # Disable memory locking (can cause issues on macOS)
                n_gpu_layers=24,     # Use GPU acceleration (BERT has 24 layers)
            )
            logger.info("BGE-M3 Korean model loaded successfully!")
            logger.info(f"Model using {cpu_cores} CPU threads with batch size 512")
            logger.info("GPU acceleration enabled with 24 layers on Metal")
        except Exception as e:
            logger.error(f"Failed to load BGE-M3 model: {e}")
            raise
    
    def encode(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load() first.")
        
        try:
            embedding = self.model.create_embedding(text)
            # Extract the embedding vector from the response
            if isinstance(embedding, dict) and 'data' in embedding:
                return embedding['data'][0]['embedding']
            return embedding
        except Exception as e:
            logger.error(f"Failed to encode text: {e}")
            raise
    
    def benchmark_embedding(self):
        """Benchmark embedding performance with different text lengths."""
        logger.info("Starting embedding benchmark...")
        test_texts = [
            "ì§§ì€ í…ìŠ¤íŠ¸",
            "ì¤‘ê°„ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë¬¸ì¥ìœ¼ë¡œ ì ë‹¹í•œ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
            "ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 20 + "ì´ í…ìŠ¤íŠ¸ëŠ” ì„ë² ë”© ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ê¸´ ë¬¸ì¥ì…ë‹ˆë‹¤."
        ]
        
        for i, text in enumerate(test_texts, 1):
            start = time.time()
            embedding = self.encode(text)
            elapsed = time.time() - start
            logger.info(f"[BENCH] Text {i} ({len(text)} chars, dim={len(embedding)}): {elapsed:.3f}s")
        
        logger.info("Embedding benchmark completed")

class KoreanBM25Retriever:
    """í•œêµ­ì–´ ìµœì í™” BM25 ê²€ìƒ‰ê¸°"""
    
    def __init__(self, tokenizer_type: str = "okt"):
        """
        tokenizer_type: 'okt', 'kkma' ì¤‘ ì„ íƒ
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ 'okt'ê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥
        """
        self.tokenizer_type = tokenizer_type
        self.tokenizer = None
        self.bm25 = None
        self.documents = []
        self.tokenized_docs = []
        self._init_tokenizer()
    
    def _init_tokenizer(self):
        """í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            if self.tokenizer_type == "okt":
                from konlpy.tag import Okt
                self.tokenizer = Okt()
                logger.info("Korean BM25: Using Okt tokenizer")
            elif self.tokenizer_type == "kkma":
                from konlpy.tag import Kkma
                self.tokenizer = Kkma()
                logger.info("Korean BM25: Using Kkma tokenizer")
            else:
                raise ValueError(f"Unsupported tokenizer: {self.tokenizer_type}")
        except Exception as e:
            logger.error(f"Failed to initialize Korean tokenizer: {e}")
            logger.error("Falling back to simple regex tokenizer")
            self.tokenizer = None
            self.tokenizer_type = "regex"
        
    def _tokenize_korean(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í°í™”"""
        if not text or not text.strip():
            return []
        
        try:
            if self.tokenizer and self.tokenizer_type != "regex":
                morphs = self.tokenizer.morphs(text)
                tokens = [word for word in morphs if len(word) > 1 and word.replace(' ', '').isalnum()]
                return tokens
            else:
                # Regex fallback tokenizer
                import re
                # í•œê¸€, ì˜ë¬¸, ìˆ«ì ì¶”ì¶œ
                tokens = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{2,}|[0-9]+', text)
                return [token for token in tokens if len(token) > 1]
        except Exception as e:
            logger.error(f"Tokenization error: {e}")
            # Last resort - simple split
            return [word for word in text.split() if len(word) > 1]
    
    def build_index(self, documents: List[str]):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not documents:
            logger.warning("No documents provided for BM25 indexing")
            return
            
        start_time = time.time()
        logger.info(f"Building BM25 index for {len(documents)} documents...")
        
        try:
            from rank_bm25 import BM25Okapi
            
            self.documents = documents
            self.tokenized_docs = []
            
            for i, doc in enumerate(documents):
                tokenized = self._tokenize_korean(doc)
                self.tokenized_docs.append(tokenized)
                if (i + 1) % 100 == 0:
                    logger.info(f"Tokenized {i + 1}/{len(documents)} documents")
            
            self.bm25 = BM25Okapi(self.tokenized_docs)
            
            elapsed = time.time() - start_time
            logger.info(f"[TIMER] BM25 index building took {elapsed:.3f}s")
            
        except ImportError as e:
            logger.error(f"Failed to import rank-bm25: {e}")
            logger.error("Please install rank-bm25: pip install rank-bm25")
            raise
        except Exception as e:
            logger.error(f"BM25 index building failed: {e}")
            raise
            
    def search(self, query: str, top_k: int = 50) -> List[tuple]:
        """BM25 ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.bm25:
            logger.error("BM25 index not built. Call build_index() first.")
            return []
        
        if not query or not query.strip():
            return []
        
        try:
            tokenized_query = self._tokenize_korean(query)
            if not tokenized_query:
                return []
                
            scores = self.bm25.get_scores(tokenized_query)
            
            doc_scores = [(i, score) for i, score in enumerate(scores) if score > 0]
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            return doc_scores[:top_k]
            
        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
            return []

class BGERerankerModel:
    """BGE-Reranker-v2-M3-Ko Korean reranker model wrapper using llama-cpp-python."""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
    
    def load(self):
        """Load the GGUF reranker model with optimized settings."""
        try:
            cpu_cores = os.cpu_count()
            logger.info(f"Loading BGE-Reranker-v2-M3-Ko from: {self.model_path}")
            
            self.model = Llama(
                model_path=self.model_path,
                embedding=False,     # Reranker mode (not embedding)
                n_ctx=8192,          # Context window
                verbose=False,       # Minimize logs
                n_threads=cpu_cores, # Use all CPU cores
                n_batch=512,         # Batch size optimization
                use_mmap=True,       # Memory mapping
                use_mlock=False,     # Disable memory locking
                n_gpu_layers=24,     # GPU acceleration
            )
            logger.info("BGE-Reranker-v2-M3-Ko model loaded successfully!")
            logger.info(f"Reranker using {cpu_cores} CPU threads with GPU acceleration")
            
        except Exception as e:
            logger.error(f"Failed to load reranker model: {e}")
            raise
    
    def rerank(self, query: str, documents: List[str], top_k: int = 10) -> List[tuple]:
        """í•œêµ­ì–´ ìµœì í™” ì¬ìˆœìœ„ ì²˜ë¦¬"""
        if not self.model:
            raise RuntimeError("Reranker model not loaded. Call load() first.")
        
        if not documents:
            return []
        
        try:
            scores = []
            for i, document in enumerate(documents):
                input_text = self._prepare_korean_input(query, document)
                
                response = self.model(
                    input_text,
                    max_tokens=1,
                    temperature=0.0,
                    logits_all=True
                )
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (logitsì—ì„œ ì¶”ì¶œ)
                if 'logits' in response:
                    logits = response['logits']
                    # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                    score = sum(logits) / len(logits) if logits else 0.0
                else:
                    score = 0.0
                
                scores.append((i, score))
            
            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:top_k]
            
        except Exception as e:
            logger.error(f"Reranking failed: {e}")
            # í´ë°±: ì›ë˜ ìˆœì„œ ë°˜í™˜
            return [(i, 1.0) for i in range(min(top_k, len(documents)))]
    
    def _prepare_korean_input(self, query: str, document: str) -> str:
        """í•œêµ­ì–´ íŠ¹í™” ì…ë ¥ ì „ì²˜ë¦¬"""
        max_query_tokens = 512
        max_doc_tokens = 7168
        
        # ê°„ë‹¨í•œ í† í° ê¸¸ì´ ì¶”ì • (í•œê¸€ 1ê¸€ì â‰ˆ 1í† í°)
        query_truncated = query[:max_query_tokens] if len(query) > max_query_tokens else query
        doc_truncated = document[:max_doc_tokens] if len(document) > max_doc_tokens else document
        
        # Cross-encoder ì…ë ¥ í˜•ì‹
        return f"[CLS] {query_truncated} [SEP] {doc_truncated} [SEP]"

class HybridFusion:
    """Dense + Sparse ê²°ê³¼ ìœµí•©"""
    
    @staticmethod
    def reciprocal_rank_fusion(dense_results: List[tuple], 
                             sparse_results: List[tuple], 
                             k: int = 60) -> List[tuple]:
        """Reciprocal Rank Fusion (RRF) ì•Œê³ ë¦¬ì¦˜"""
        fusion_scores = {}
        
        # Dense retrieval ì ìˆ˜ ì •ê·œí™” (doc_id, score í˜•íƒœ ê°€ì •)
        for rank, (doc_id, score) in enumerate(dense_results, 1):
            fusion_scores[doc_id] = 1.0 / (k + rank)
            
        # Sparse retrieval ì ìˆ˜ ì¶”ê°€
        for rank, (doc_id, score) in enumerate(sparse_results, 1):
            if doc_id in fusion_scores:
                fusion_scores[doc_id] += 1.0 / (k + rank)
            else:
                fusion_scores[doc_id] = 1.0 / (k + rank)
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        fused = sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)
        return fused

class WatchFilesMonitor:
    """Real-time file monitoring using watchfiles library."""
    
    def __init__(self, documents_path: str, config: Dict[str, Any], text_extractor=None):
        self.documents_path = Path(documents_path)
        self.config = config.get('watchfiles', {})
        self.text_extractor = text_extractor
        self.ignore_paths = set(self.config.get('ignore_paths', ['.git', '__pycache__', '.DS_Store']))
        self.debounce_ms = self.config.get('debounce_ms', 100)
        self.batch_delay_ms = self.config.get('batch_delay_ms', 500)
        self.is_monitoring = False
        self.monitor_task = None
        self.change_callback = None
        logger.info(f"WatchFilesMonitor initialized for: {self.documents_path}")
    
    def set_change_callback(self, callback):
        """Set callback function for file change events."""
        self.change_callback = callback
        logger.info("File change callback registered")
    
    def should_process_file(self, file_path: Path) -> bool:
        """Check if file should be processed based on extension and ignore rules."""
        # Check extension using text extractor's supported formats
        if self.text_extractor:
            supported_extensions = self.text_extractor.get_supported_extensions()
            if file_path.suffix.lower() not in supported_extensions:
                return False
        else:
            # Fallback to basic text files if no text extractor available
            if file_path.suffix.lower() not in {'.txt', '.md'}:
                return False
        
        # Check ignore paths
        for ignore in self.ignore_paths:
            if ignore in str(file_path):
                return False
                
        return True
    
    async def start_monitoring(self):
        """Start the file monitoring task."""
        if self.is_monitoring:
            logger.warning("File monitoring already started")
            return
            
        if not self.config.get('enabled', True):
            logger.info("WatchFiles monitoring disabled in config")
            return
            
        try:
            from watchfiles import awatch
            logger.info(f"Starting watchfiles monitoring on: {self.documents_path}")
            self.is_monitoring = True
            
            # Start monitoring task
            self.monitor_task = asyncio.create_task(self._monitor_loop())
            logger.info("âœ… WatchFiles monitoring started successfully")
            
        except ImportError:
            logger.error("watchfiles library not installed. Install with: pip install watchfiles")
        except Exception as e:
            logger.error(f"Failed to start file monitoring: {e}")
            self.is_monitoring = False
    
    async def stop_monitoring(self):
        """Stop the file monitoring task."""
        if not self.is_monitoring:
            return
            
        self.is_monitoring = False
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("WatchFiles monitoring stopped")
    
    async def _monitor_loop(self):
        """Main monitoring loop using watchfiles."""
        try:
            from watchfiles import awatch, Change
            
            logger.info("WatchFiles monitoring loop started")
            batch_changes = []
            last_change_time = 0
            
            async for changes in awatch(self.documents_path, recursive=self.config.get('watch_recursive', True)):
                current_time = time.time() * 1000  # milliseconds
                
                # Filter and categorize changes
                for change_type, file_path in changes:
                    file_path = Path(file_path)
                    
                    if not self.should_process_file(file_path):
                        continue
                    
                    change_info = {
                        'type': self._get_change_type(change_type),
                        'path': file_path,
                        'timestamp': current_time
                    }
                    batch_changes.append(change_info)
                    last_change_time = current_time
                
                # Debounce: wait for batch_delay_ms after last change
                if batch_changes:
                    await asyncio.sleep(self.batch_delay_ms / 1000)
                    
                    # Check if more changes arrived during delay
                    if (time.time() * 1000) - last_change_time >= self.batch_delay_ms:
                        await self._process_changes(batch_changes)
                        batch_changes.clear()
                        
        except asyncio.CancelledError:
            logger.info("WatchFiles monitoring cancelled")
            raise
        except Exception as e:
            logger.error(f"Error in watchfiles monitoring loop: {e}")
            self.is_monitoring = False
    
    def _get_change_type(self, watchfiles_change):
        """Convert watchfiles change type to our format."""
        try:
            from watchfiles import Change
            if watchfiles_change == Change.added:
                return 'added'
            elif watchfiles_change == Change.modified:
                return 'modified'
            elif watchfiles_change == Change.deleted:
                return 'deleted'
            else:
                return 'unknown'
        except ImportError:
            return 'modified'  # fallback
    
    async def _process_changes(self, changes):
        """Process batched file changes."""
        if not changes:
            return
            
        logger.info(f"Processing {len(changes)} file changes")
        
        # Group changes by type
        changes_by_type = {'added': [], 'modified': [], 'deleted': []}
        for change in changes:
            change_type = change['type']
            if change_type in changes_by_type:
                changes_by_type[change_type].append(change['path'])
        
        # Call callback if registered
        if self.change_callback:
            try:
                await self.change_callback(changes_by_type)
            except Exception as e:
                logger.error(f"Error in file change callback: {e}")
        
        # Log changes
        for change_type, paths in changes_by_type.items():
            if paths:
                logger.info(f"ğŸ“ {change_type.title()}: {len(paths)} files")
                for path in paths[:3]:  # Show first 3 files
                    logger.info(f"   - {path}")
                if len(paths) > 3:
                    logger.info(f"   ... and {len(paths) - 3} more files")


class RAGServer:
    """RAG MCP Server with embedding model."""
    
    def __init__(self):
        self.embedding_model = None
        self.reranker_model = None
        self.bm25_retriever = None
        self.config = {}
        self.file_monitor = None
        self.text_extractor = None  # Multi-format text extraction engine
        self.vector_store = None  # ChromaDB vector storage
        self.documents = []  # Store documents for BM25 and reranker
        self.model_path = Path(__file__).parent / ".model" / "bge-m3-korean-q4_k_m-2.gguf"
        self.reranker_path = Path(__file__).parent / ".model" / "bge-reranker-v2-m3-ko-q4_k_m.gguf"
        self.config_path = Path(__file__).parent / "config.json"
        self.documents_path = Path(__file__).parent / "documents"
    
    def load_config(self):
        """Load configuration from config.json."""
        start_time = time.time()
        try:
            if self.config_path.exists():
                logger.info(f"Loading configuration from: {self.config_path}")
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                logger.info(f"Configuration loaded: {list(self.config.keys())}")
            else:
                logger.info("No config.json found, using default settings")
                self.config = {}
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            self.config = {}
        finally:
            elapsed = time.time() - start_time
            logger.info(f"[TIMER] Configuration loading took {elapsed:.3f}s")

    async def initialize_model(self):
        """Initialize the embedding model."""
        total_start = time.time()
        try:
            # Load configuration first
            config_start = time.time()
            self.load_config()
            
            # Initialize text extraction engine
            extractor_start = time.time()
            logger.info("Initializing text extraction engine...")
            extraction_config = self.config.get('text_extraction', {
                'max_workers': min(32, os.cpu_count() + 4),
                'max_file_size': 100 * 1024 * 1024,  # 100MB
                'use_multiprocessing': False
            })
            self.text_extractor = TextExtractionEngine(extraction_config)
            extractor_elapsed = time.time() - extractor_start
            logger.info(f"[TIMER] Text extraction engine initialization took {extractor_elapsed:.3f}s")
            
            # Initialize ChromaDB vector store
            vector_start = time.time()
            logger.info("Initializing ChromaDB vector store...")
            try:
                vector_config = self.config.get('vector_store', {})
                db_path = vector_config.get('db_path', '.vectordb')
                collection_name = vector_config.get('collection_name', 'rag_documents')
                
                self.vector_store = ChromaDBVectorStore(
                    db_path=db_path,
                    collection_name=collection_name
                )
                logger.info(f"ChromaDB vector store initialized at: {db_path}")
                
                # Get collection stats
                stats = self.vector_store.get_collection_stats()
                logger.info(f"Vector store stats: {stats}")
                
            except ImportError:
                logger.warning("ChromaDB not available, falling back to naive dense search")
                self.vector_store = None
            except Exception as e:
                logger.error(f"Failed to initialize vector store: {e}")
                logger.warning("Falling back to naive dense search")
                self.vector_store = None
                
            vector_elapsed = time.time() - vector_start
            logger.info(f"[TIMER] Vector store initialization took {vector_elapsed:.3f}s")
            
            # Initialize file monitor
            monitor_start = time.time()
            logger.info("Initializing watchfiles monitor...")
            self.file_monitor = WatchFilesMonitor(str(self.documents_path), self.config, self.text_extractor)
            self.file_monitor.set_change_callback(self._handle_file_changes)
            monitor_elapsed = time.time() - monitor_start
            logger.info(f"[TIMER] File monitor initialization took {monitor_elapsed:.3f}s")
            
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model file not found: {self.model_path}")
            
            # Load embedding model
            model_start = time.time()
            logger.info("Initializing BGE-M3 Korean embedding model...")
            self.embedding_model = BGEEmbeddingModel(str(self.model_path))
            self.embedding_model.load()
            model_elapsed = time.time() - model_start
            logger.info(f"[TIMER] Model loading took {model_elapsed:.3f}s")
            
            # Test model with a simple embedding
            test_start = time.time()
            logger.info("Testing embedding model...")
            try:
                test_text = "ì•ˆë…•í•˜ì„¸ìš”"
                test_embedding = self.embedding_model.encode(test_text)
                logger.info(f"Model test successful! Embedding dimension: {len(test_embedding)}")
                
                # Run benchmark after successful test
                self.embedding_model.benchmark_embedding()
                
            except Exception as e:
                logger.error(f"Model test failed: {e}")
                # Continue without failing - model is loaded, just test failed
                logger.info("Continuing without embedding test...")
            test_elapsed = time.time() - test_start
            logger.info(f"[TIMER] Model testing took {test_elapsed:.3f}s")
            
            # Load reranker model if available
            reranker_start = time.time()
            if self.reranker_path.exists():
                logger.info("Initializing BGE-Reranker-v2-M3-Ko model...")
                try:
                    self.reranker_model = BGERerankerModel(str(self.reranker_path))
                    self.reranker_model.load()
                    logger.info("Reranker model loaded successfully!")
                except Exception as e:
                    logger.error(f"Failed to load reranker model: {e}")
                    logger.info("Continuing without reranker model...")
                    self.reranker_model = None
            else:
                logger.info(f"Reranker model not found: {self.reranker_path}")
                logger.info("Continuing without reranker model...")
                self.reranker_model = None
            reranker_elapsed = time.time() - reranker_start
            logger.info(f"[TIMER] Reranker initialization took {reranker_elapsed:.3f}s")
            
            # Initialize BM25 retriever
            bm25_start = time.time()
            logger.info("Initializing Korean BM25 retriever...")
            try:
                tokenizer_type = self.config.get('bm25', {}).get('tokenizer', 'okt')
                self.bm25_retriever = KoreanBM25Retriever(tokenizer_type=tokenizer_type)
                logger.info("BM25 retriever initialized successfully!")
            except Exception as e:
                logger.error(f"Failed to initialize BM25 retriever: {e}")
                logger.info("Continuing without BM25 retriever...")
                self.bm25_retriever = None
            bm25_elapsed = time.time() - bm25_start
            logger.info(f"[TIMER] BM25 initialization took {bm25_elapsed:.3f}s")
            
            # Initial scan and extraction of documents directory
            scan_start = time.time()
            logger.info("Performing initial document extraction...")
            self.documents = []  # Reset document list
            
            # Extract text from all supported documents using TextExtractionEngine
            try:
                def progress_callback(processed, total, result):
                    if processed % 10 == 0 or processed == total:
                        logger.info(f"Extracted {processed}/{total} documents...")
                
                extraction_result = self.text_extractor.extract_directory(
                    self.documents_path,
                    recursive=True,
                    include_hidden=False,
                    progress_callback=progress_callback
                )
                
                # Process successful extractions and populate vector store
                vector_docs = []
                for doc in extraction_result.documents:
                    if doc.success and doc.content.strip():
                        # Use cleaned content for RAG
                        clean_content = doc.get_clean_content()
                        if clean_content:
                            self.documents.append(clean_content)
                            
                            # Prepare document for vector store
                            if self.vector_store:
                                try:
                                    embedding = self.embedding_model.encode(clean_content[:1000])  # Limit context for embedding
                                    
                                    # Create metadata
                                    metadata = self.vector_store.create_document_metadata(
                                        file_path=doc.file_path,
                                        file_type=str(doc.metadata.file_type.value) if hasattr(doc.metadata.file_type, 'value') else str(doc.metadata.file_type),
                                        extractor_type=doc.metadata.extractor_name,
                                        chunk_index=0
                                    )
                                    
                                    vector_docs.append({
                                        'content': clean_content,
                                        'file_path': doc.file_path,
                                        'embedding': embedding,
                                        'metadata': metadata
                                    })
                                    
                                except Exception as e:
                                    logger.warning(f"Failed to prepare vector document for {doc.file_path}: {e}")
                        else:
                            logger.warning(f"No clean content extracted from {doc.file_path}")
                    elif not doc.success:
                        logger.warning(f"Failed to extract {doc.file_path}: {doc.error_message}")
                
                logger.info(f"Extracted text from {len(self.documents)} documents")
                logger.info(f"Success rate: {extraction_result.success_rate:.1f}%")
                
                # Populate ChromaDB vector store with initial documents using efficient batch upsert
                if self.vector_store and vector_docs:
                    try:
                        vector_start = time.time()
                        
                        # Create VectorDocument objects for batch upsert
                        from vector_store.chroma_store import VectorDocument
                        vector_documents = []
                        
                        for doc_data in vector_docs:
                            vector_doc = VectorDocument(
                                id="",  # Let the vector store generate the ID
                                content=doc_data['content'],
                                embedding=doc_data['embedding'],
                                metadata=doc_data['metadata']
                            )
                            vector_documents.append(vector_doc)
                        
                        # Efficient batch upsert
                        upserted_ids = self.vector_store.upsert_documents(vector_documents)
                        
                        vector_elapsed = time.time() - vector_start
                        logger.info(f"ğŸ—‚ï¸  Populated ChromaDB with {len(upserted_ids)} documents (batch upsert)")
                        logger.info(f"[TIMER] Initial vector store population took {vector_elapsed:.3f}s")
                        
                        # Log vector store stats
                        stats = self.vector_store.get_collection_stats()
                        logger.info(f"ğŸ“Š Vector store total documents: {stats['total_documents']}")
                        
                    except Exception as e:
                        logger.error(f"Failed to populate vector store: {e}")
                
                # Log extraction statistics
                stats = self.text_extractor.get_extraction_stats()
                logger.info(f"Text extraction engine stats: {stats['total_extractors']} extractors, "
                           f"supports {len(stats['supported_extensions'])} file types")
                
            except Exception as e:
                logger.error(f"Failed to extract documents: {e}")
                self.documents = []
            
            scan_elapsed = time.time() - scan_start
            logger.info(f"[TIMER] Initial document extraction took {scan_elapsed:.3f}s")
            
            # Build BM25 index if documents available and BM25 retriever initialized
            if self.documents and self.bm25_retriever:
                index_start = time.time()
                logger.info("Building BM25 index from loaded documents...")
                try:
                    self.bm25_retriever.build_index(self.documents)
                    logger.info("BM25 index built successfully!")
                except Exception as e:
                    logger.error(f"BM25 index building failed: {e}")
                index_elapsed = time.time() - index_start
                logger.info(f"[TIMER] BM25 indexing took {index_elapsed:.3f}s")
            
            total_elapsed = time.time() - total_start
            logger.info(f"[TIMER] TOTAL initialization took {total_elapsed:.3f}s")
            
        except Exception as e:
            logger.error(f"Model initialization failed: {e}")
            raise
    
    async def _dense_search(self, query: str, top_k: int = 50) -> List[tuple]:
        """Dense retrieval using BGE-M3 embedding with ChromaDB or fallback to naive search"""
        if not self.embedding_model:
            return []
        
        try:
            query_embedding = self.embedding_model.encode(query)
            
            # Use ChromaDB if available
            if self.vector_store:
                results = self.vector_store.search(
                    query_embeddings=[query_embedding],
                    n_results=min(top_k, 100),  # Reasonable limit
                    include=['documents', 'metadatas', 'distances']
                )
                
                # Convert ChromaDB results to document indices
                scores = []
                if results.get('ids') and results.get('distances'):
                    for i, (doc_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                        # Convert distance to similarity score (ChromaDB returns cosine distance)
                        similarity = 1 - distance
                        
                        # Find document index in self.documents by content matching
                        if results.get('documents') and i < len(results['documents'][0]):
                            doc_content = results['documents'][0][i]
                            # Find matching document in self.documents
                            for doc_idx, doc in enumerate(self.documents):
                                if doc.startswith(doc_content[:100]):  # Match by content prefix
                                    scores.append((doc_idx, similarity))
                                    break
                
                return scores[:top_k]
            
            # Fallback to naive search if ChromaDB not available
            if not self.documents:
                return []
                
            scores = []
            for i, doc in enumerate(self.documents):
                doc_embedding = self.embedding_model.encode(doc[:1000])  # Truncate for speed
                
                # Cosine similarity
                dot_product = sum(a * b for a, b in zip(query_embedding, doc_embedding))
                norm_a = sum(a * a for a in query_embedding) ** 0.5
                norm_b = sum(b * b for b in doc_embedding) ** 0.5
                
                if norm_a > 0 and norm_b > 0:
                    similarity = dot_product / (norm_a * norm_b)
                    scores.append((i, similarity))
            
            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:top_k]
            
        except Exception as e:
            logger.error(f"Dense search failed: {e}")
            return []
    
    async def _handle_file_changes(self, changes: Dict[str, List]):
        """Handle file changes detected by watchfiles using TextExtractionEngine."""
        logger.info("ğŸ”„ File changes detected, updating document index...")
        
        # Re-extract all documents when changes are detected
        reload_start = time.time()
        try:
            self.documents = []
            
            def progress_callback(processed, total, result):
                if processed % 5 == 0 or processed == total:
                    logger.info(f"Re-extracted {processed}/{total} documents...")
            
            # Use TextExtractionEngine to re-extract all documents
            extraction_result = self.text_extractor.extract_directory(
                self.documents_path,
                recursive=True,
                include_hidden=False,
                progress_callback=progress_callback
            )
            
            # Process successful extractions and update vector store
            vector_docs = []
            for doc in extraction_result.documents:
                if doc.success and doc.content.strip():
                    clean_content = doc.get_clean_content()
                    if clean_content:
                        self.documents.append(clean_content)
                        
                        # Prepare document for vector store
                        if self.vector_store and self.embedding_model:
                            try:
                                embedding = self.embedding_model.encode(clean_content[:1000])  # Limit context for embedding
                                
                                # Create metadata
                                metadata = self.vector_store.create_document_metadata(
                                    file_path=doc.file_path,
                                    file_type=str(doc.metadata.file_type.value) if hasattr(doc.metadata.file_type, 'value') else str(doc.metadata.file_type),
                                    extractor_type=doc.metadata.extractor_name,
                                    chunk_index=0
                                )
                                
                                vector_docs.append({
                                    'content': clean_content,
                                    'file_path': doc.file_path,
                                    'embedding': embedding,
                                    'metadata': metadata
                                })
                                
                            except Exception as e:
                                logger.warning(f"Failed to prepare vector document for {doc.file_path}: {e}")
            
            logger.info(f"ğŸ“š Re-extracted {len(self.documents)} documents")
            logger.info(f"ğŸ“Š Extraction success rate: {extraction_result.success_rate:.1f}%")
            
            # Update ChromaDB vector store with efficient batch upsert
            if self.vector_store and vector_docs:
                try:
                    vector_start = time.time()
                    
                    # Create VectorDocument objects for batch upsert
                    from vector_store.chroma_store import VectorDocument
                    vector_documents = []
                    
                    for doc_data in vector_docs:
                        vector_doc = VectorDocument(
                            id="",  # Let the vector store generate the ID
                            content=doc_data['content'],
                            embedding=doc_data['embedding'],
                            metadata=doc_data['metadata']
                        )
                        vector_documents.append(vector_doc)
                    
                    # Efficient batch upsert
                    upserted_ids = self.vector_store.upsert_documents(vector_documents)
                    
                    vector_elapsed = time.time() - vector_start
                    logger.info(f"ğŸ—‚ï¸  Updated {len(upserted_ids)} documents in ChromaDB vector store (batch upsert)")
                    logger.info(f"[TIMER] Vector store update took {vector_elapsed:.3f}s")
                    
                    # Log vector store stats
                    stats = self.vector_store.get_collection_stats()
                    logger.info(f"ğŸ“Š Vector store total documents: {stats['total_documents']}")
                    
                except Exception as e:
                    logger.error(f"Failed to update vector store: {e}")
            
            # Rebuild BM25 index if available
            if self.documents and self.bm25_retriever:
                self.bm25_retriever.build_index(self.documents)
                logger.info("ğŸ” BM25 index rebuilt")
                
        except Exception as e:
            logger.error(f"Failed to handle file changes: {e}")
        
        reload_elapsed = time.time() - reload_start
        logger.info(f"[TIMER] File change processing took {reload_elapsed:.3f}s")
    
    async def start_monitoring(self):
        """Start file monitoring."""
        if self.file_monitor:
            await self.file_monitor.start_monitoring()
    
    async def stop_monitoring(self):
        """Stop file monitoring."""
        if self.file_monitor:
            await self.file_monitor.stop_monitoring()
    
    async def hybrid_search(self, query: str, top_k: int = 10) -> List[Dict]:
        """3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Dense + Sparse + Reranking"""
        if not self.documents:
            return []
        
        search_start = time.time()
        logger.info(f"Starting hybrid search for query: '{query[:50]}...'")
        
        # Get configuration
        config = self.config.get('hybrid_search', {})
        dense_top_k = config.get('dense_top_k', 50)
        sparse_top_k = config.get('sparse_top_k', 50)
        rerank_candidates = config.get('rerank_candidates', 30)
        
        # 1ë‹¨ê³„: Dense Retrieval (BGE-M3)
        dense_start = time.time()
        logger.info("Stage 1: Dense retrieval...")
        dense_results = await self._dense_search(query, top_k=dense_top_k)
        dense_elapsed = time.time() - dense_start
        logger.info(f"[TIMER] Dense retrieval took {dense_elapsed:.3f}s, found {len(dense_results)} results")
        
        # 2ë‹¨ê³„: Sparse Retrieval (BM25)
        sparse_start = time.time()
        logger.info("Stage 2: Sparse retrieval...")
        sparse_results = []
        if self.bm25_retriever:
            sparse_results = self.bm25_retriever.search(query, top_k=sparse_top_k)
        sparse_elapsed = time.time() - sparse_start
        logger.info(f"[TIMER] Sparse retrieval took {sparse_elapsed:.3f}s, found {len(sparse_results)} results")
        
        # 3ë‹¨ê³„: Fusion & Reranking
        fusion_start = time.time()
        logger.info("Stage 3: Fusion and reranking...")
        
        if not dense_results and not sparse_results:
            logger.info("No results from dense or sparse retrieval")
            return []
        
        # ê²°ê³¼ ìœµí•© (RRF)
        if dense_results and sparse_results:
            fused_results = HybridFusion.reciprocal_rank_fusion(dense_results, sparse_results)
        elif dense_results:
            fused_results = dense_results
        else:
            fused_results = sparse_results
        
        # ìƒìœ„ candidatesë¥¼ ë¦¬ë­ì»¤ì— ì „ë‹¬
        top_candidates = fused_results[:rerank_candidates]
        
        # Reranking with Korean optimization
        final_results = []
        if self.reranker_model and len(top_candidates) > 1:
            try:
                candidate_docs = [self.documents[doc_id] for doc_id, _ in top_candidates]
                reranked = self.reranker_model.rerank(query, candidate_docs, top_k=top_k)
                
                # ê²°ê³¼ í¬ë§·íŒ…
                for rank_idx, (orig_idx, score) in enumerate(reranked):
                    if rank_idx < len(top_candidates):
                        doc_id, fusion_score = top_candidates[orig_idx]
                        final_results.append({
                            'rank': rank_idx + 1,
                            'doc_id': doc_id,
                            'content': self.documents[doc_id][:500] + '...' if len(self.documents[doc_id]) > 500 else self.documents[doc_id],
                            'rerank_score': score,
                            'fusion_score': fusion_score,
                            'method': 'hybrid_reranked'
                        })
                        
            except Exception as e:
                logger.error(f"Reranking failed: {e}")
                # í´ë°±: fusion ê²°ê³¼ë§Œ ì‚¬ìš©
                for i, (doc_id, score) in enumerate(top_candidates[:top_k]):
                    final_results.append({
                        'rank': i + 1,
                        'doc_id': doc_id,
                        'content': self.documents[doc_id][:500] + '...' if len(self.documents[doc_id]) > 500 else self.documents[doc_id],
                        'fusion_score': score,
                        'method': 'hybrid_fusion_only'
                    })
        else:
            # Reranker ì—†ì´ fusion ê²°ê³¼ë§Œ ì‚¬ìš©
            for i, (doc_id, score) in enumerate(top_candidates[:top_k]):
                final_results.append({
                    'rank': i + 1,
                    'doc_id': doc_id,
                    'content': self.documents[doc_id][:500] + '...' if len(self.documents[doc_id]) > 500 else self.documents[doc_id],
                    'fusion_score': score,
                    'method': 'hybrid_fusion_only'
                })
        
        fusion_elapsed = time.time() - fusion_start
        logger.info(f"[TIMER] Fusion & reranking took {fusion_elapsed:.3f}s")
        
        total_elapsed = time.time() - search_start
        logger.info(f"[TIMER] Total hybrid search took {total_elapsed:.3f}s, returning {len(final_results)} results")
        
        return final_results

# Create server instances
rag_server = RAGServer()
app = Server("rag-mcp")

@app.list_resources()
async def handle_list_resources() -> list[Resource]:
    """List available resources."""
    return [
        Resource(
            uri="rag://documents",
            name="Document Collection",
            description="Access to RAG document collection",
            mimeType="application/json",
        )
    ]

@app.read_resource()
async def handle_read_resource(uri: str) -> str:
    """Read a specific resource."""
    if uri == "rag://documents":
        return "RAG document collection placeholder - implementation coming soon!"
    else:
        raise ValueError(f"Unknown resource: {uri}")

@app.list_tools()
async def handle_list_tools() -> list[Tool]:
    """List available tools."""
    return [
        Tool(
            name="search_documents",
            description="Search through document collection using semantic similarity",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "Maximum number of results",
                        "default": 5
                    }
                },
                "required": ["query"]
            }
        )
    ]

@app.call_tool()
async def handle_call_tool(name: str, arguments: dict[str, Any] | None) -> list[TextContent]:
    """Handle tool calls."""
    if name == "search_documents":
        query = arguments.get("query") if arguments else ""
        limit = arguments.get("limit", 5) if arguments else 5
        
        if not query.strip():
            return [TextContent(type="text", text="âŒ Query cannot be empty")]
        
        try:
            # Use hybrid search (Dense + Sparse + Reranking)
            search_results = await rag_server.hybrid_search(query, top_k=limit)
            
            if not search_results:
                return [TextContent(type="text", text=f"ğŸ” No results found for query: '{query}'")]
            
            # Format results for display
            results_text = f"ğŸ” Hybrid Search Results for: '{query}'\n"
            results_text += f"ğŸ“Š 3-Stage Pipeline: Dense (BGE-M3) + Sparse (BM25) + Reranking (BGE-Reranker-v2-M3-Ko)\n"
            results_text += f"ğŸ¯ Found {len(search_results)} results\n\n"
            
            for result in search_results:
                results_text += f"ğŸ“„ Rank #{result['rank']}\n"
                results_text += f"   Method: {result['method']}\n"
                
                if 'rerank_score' in result:
                    results_text += f"   Rerank Score: {result['rerank_score']:.4f}\n"
                if 'fusion_score' in result:
                    results_text += f"   Fusion Score: {result['fusion_score']:.4f}\n"
                
                results_text += f"   Content: {result['content']}\n\n"
            
            # Add pipeline status
            pipeline_status = "\nğŸ”§ Pipeline Status:\n"
            pipeline_status += f"   âœ… Dense Retrieval (BGE-M3): {'Available' if rag_server.embedding_model else 'Unavailable'}\n"
            pipeline_status += f"   âœ… Sparse Retrieval (BM25): {'Available' if rag_server.bm25_retriever else 'Unavailable'}\n"
            pipeline_status += f"   âœ… Reranking (BGE-Reranker): {'Available' if rag_server.reranker_model else 'Unavailable'}\n"
            pipeline_status += f"   ğŸ“š Document Count: {len(rag_server.documents)}"
            
            results_text += pipeline_status
            
            return [TextContent(type="text", text=results_text)]
            
        except Exception as e:
            error_msg = f"âŒ Hybrid search failed: {str(e)}"
            logger.error(error_msg)
            return [TextContent(type="text", text=error_msg)]
    else:
        raise ValueError(f"Unknown tool: {name}")

async def main():
    """Main server entry point."""
    main_start = time.time()
    logger.info("Starting RAG MCP Server...")
    
    # Initialize embedding model before starting server
    try:
        init_start = time.time()
        await rag_server.initialize_model()
        init_elapsed = time.time() - init_start
        logger.info(f"[TIMER] Server initialization completed in {init_elapsed:.3f}s")
        
        # Start file monitoring
        monitor_start = time.time()
        await rag_server.start_monitoring()
        monitor_elapsed = time.time() - monitor_start
        logger.info(f"[TIMER] File monitoring started in {monitor_elapsed:.3f}s")
        
    except Exception as e:
        logger.error(f"Failed to initialize server: {e}")
        return
    
    server_start = time.time()
    logger.info("Server running on stdio - ready to accept MCP requests")
    
    async with stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="rag-mcp",
                server_version="0.1.0",
                capabilities=ServerCapabilities(
                    resources={},
                    tools={},
                    prompts={}
                )
            )
        )
    
    total_elapsed = time.time() - main_start
    logger.info(f"[TIMER] TOTAL server runtime: {total_elapsed:.3f}s")

if __name__ == "__main__":
    asyncio.run(main())